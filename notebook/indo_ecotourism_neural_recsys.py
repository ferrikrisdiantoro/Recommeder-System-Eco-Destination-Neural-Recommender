# -*- coding: utf-8 -*-
"""indo_ecotourism_neural_recsys.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18EKGO93yOmQxMg2prWCGIO2PE8_mGmgj

# **Indo Ecotourism — Neural Recommender (TensorFlow Recommenders)**

## **[1] Setup & Install**
"""

!pip -q install -U tensorflow tensorflow-recommenders

import os, re, random, math, numpy as np, pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import tensorflow as tf, tensorflow_recommenders as tfrs

print("TF:", tf.__version__, "| TFRS:", tfrs.__version__)
SEED = 42
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

BASE_DIR = Path.cwd()
ART_DIR  = BASE_DIR / "artifacts"
ART_DIR.mkdir(parents=True, exist_ok=True)

"""## **[2] Load Data + EDA Ringkas**"""

items_path = ART_DIR / "items.csv"
if not items_path.exists():
    raise FileNotFoundError("artifacts/items.csv tidak ditemukan. Siapkan dari notebook CBF.")

items = pd.read_csv(items_path)

# Lengkapi kolom penting untuk UI/filter
for col in ["place_name","category","city","rating","price","place_img","place_map","place_description"]:
    if col not in items.columns: items[col] = np.nan

# gabungan teks (fallback)
if "gabungan" not in items.columns:
    def _mk_text(r):
        parts = [str(r.get("place_description","")),
                 str(r.get("category","")),
                 str(r.get("city","")),
                 str(r.get("place_name",""))]
        return " ".join([p for p in parts if isinstance(p,str)]).strip()
    items["gabungan"] = items.apply(_mk_text, axis=1)

# item_id string
items["item_id"] = items["item_id"].astype(str) if "item_id" in items.columns else items.index.astype(str)

# bersihkan
items["gabungan"] = items["gabungan"].astype(str).str.strip()
items = items.dropna(subset=["gabungan"]).reset_index(drop=True)

print("=== ITEMS ===")
print("items shape:", items.shape)
print("unique item_id:", items["item_id"].nunique(), "| duplicates item_id:", len(items) - items["item_id"].nunique())
print("empty 'gabungan':", int((items["gabungan"].str.len()==0).sum()))
print("avg desc length (chars):", float(items["gabungan"].str.len().mean()))

# Visual ringkas (opsional)
fig, ax = plt.subplots(1, 3, figsize=(15,4))
items["rating"].astype(float).plot(kind="hist", bins=20, ax=ax[0], title="Distribusi Rating"); ax[0].set_xlabel("rating")
items["category"].fillna("UNKNOWN").str.split(",").str[0].value_counts().head(10).plot(kind="bar", ax=ax[1], title="Top-10 Kategori"); ax[1].tick_params(axis='x', rotation=45)
items["gabungan"].str.len().plot(kind="hist", bins=20, ax=ax[2], title="Panjang Teks (chars)"); ax[2].set_xlabel("chars")
plt.tight_layout(); plt.show()

"""## **[3] Pairs: beberapa query per item (structured + keywords)**"""

def q_struct_v1(row):
    cat = str(row.get("category","")).split(",")[0].strip()
    city= str(row.get("city","")).strip()
    name= str(row.get("place_name","")).strip()
    toks = [w for w in [cat, city, name] if w]
    return " ".join(toks).strip()

def q_struct_v2(row):
    cat = str(row.get("category","")).split(",")[0].strip().lower()
    city= str(row.get("city","")).strip().lower()
    q = "wisata"
    if cat:  q += f" {cat}"
    if city: q += f" di {city}"
    return q.strip()

def take_keywords(s, k):
    toks = re.findall(r"\w+", str(s).lower())
    toks = [t for t in toks if len(t) > 2]
    random.shuffle(toks)
    return " ".join(toks[:k]) if toks else ""

rows = []
for _, r in items.iterrows():
    iid = r["item_id"]; g = r["gabungan"]
    rows += [
        {"query_text": q_struct_v1(r), "item_id": iid},
        {"query_text": q_struct_v2(r), "item_id": iid},
    ]
    for k in (6, 8, 10):
        rows.append({"query_text": take_keywords(g, k=k), "item_id": iid})

pairs = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)
pairs = pairs[pairs["query_text"].astype(str).str.len() > 0].reset_index(drop=True)

# split row-wise
msk      = np.random.RandomState(SEED).rand(len(pairs)) < 0.90
train_df = pairs[msk].reset_index(drop=True)
val_df   = pairs[~msk].reset_index(drop=True)

print("=== PAIRS ===")
print("pairs shape:", pairs.shape, "| unique item_id in pairs:", pairs["item_id"].nunique())
print("train/val:", train_df.shape, val_df.shape)
print(pairs.head(5))

"""## **[4] Vectorizer bersama + Two-Tower**"""

from tensorflow.keras import layers, models

EMB_DIM    = 128
MAX_TOKENS = 40_000
SEQ_LEN    = 64   # samakan untuk user & item agar tak perlu slicing

vec = layers.TextVectorization(
    max_tokens=MAX_TOKENS,
    standardize="lower_and_strip_punctuation",
    output_mode="int",
    output_sequence_length=SEQ_LEN,
    name="unified_vec",
)

adapt_corpus = pd.concat(
    [train_df["query_text"].astype(str), items["gabungan"].astype(str)],
    axis=0, ignore_index=True
).values
vec.adapt(tf.data.Dataset.from_tensor_slices(adapt_corpus).batch(1024))
vocab_size = vec.vocabulary_size()
print("vocab_size:", vocab_size)

shared_emb = layers.Embedding(
    input_dim=vocab_size, output_dim=EMB_DIM, mask_zero=True, name="shared_emb"
)

def make_tower(name: str):
    inp = layers.Input(shape=(), dtype=tf.string, name=f"{name}_text")
    tok = vec(inp)
    emb = shared_emb(tok)
    x   = layers.GlobalAveragePooling1D(name=f"{name}_avg")(emb)
    x   = layers.Dense(EMB_DIM, activation="relu", name=f"{name}_dense1")(x)
    x   = layers.Dropout(0.15, name=f"{name}_drop")(x)
    x   = layers.Dense(EMB_DIM, activation=None, name=f"{name}_dense2")(x)
    x   = layers.LayerNormalization(name=f"{name}_ln")(x)
    x   = layers.Activation("relu")(x)
    x   = layers.UnitNormalization(axis=-1, name=f"{name}_unitnorm")(x)  # <- ganti Lambda l2_normalize
    return models.Model(inp, x, name=f"{name}_tower")

user_model = make_tower("user")
item_model = make_tower("item")

"""## **[5] Dataset TF + Training**"""

# ===== [TRAINING CELL — clean & professional logs] =====

# 1) Siapkan pasangan train/val
pair_train = train_df.merge(
    items[["item_id", "gabungan"]], on="item_id", how="left"
).rename(columns={"gabungan": "item_text"})
pair_val = val_df.merge(
    items[["item_id", "gabungan"]], on="item_id", how="left"
).rename(columns={"gabungan": "item_text"})

pair_train = pair_train.dropna(subset=["query_text", "item_text"]).reset_index(drop=True)
pair_val   = pair_val.dropna(subset=["query_text", "item_text"]).reset_index(drop=True)

# 2) Dataset tf.data
BATCH_SIZE = 256

def to_ds(df, shuffle=True):
    ds = tf.data.Dataset.from_tensor_slices({
        "query_text": df["query_text"].astype(str).values,
        "item_text":  df["item_text"].astype(str).values,
    })
    if shuffle:
        ds = ds.shuffle(min(20_000, len(df)), seed=SEED, reshuffle_each_iteration=True)
    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

train_ds = to_ds(pair_train, shuffle=True)
val_ds   = to_ds(pair_val,   shuffle=False)

# 3) Model TFRS Retrieval (in-batch negatives) + loss rata-rata
mean_ce = tf.keras.losses.CategoricalCrossentropy(
    from_logits=True,
    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE
)

class RetrievalModel(tfrs.models.Model):
    def __init__(self, user_model, item_model):
        super().__init__()
        self.user_model = user_model
        self.item_model = item_model
        self.task = tfrs.tasks.Retrieval(loss=mean_ce, metrics=None)

    def compute_loss(self, features, training=False):
        u = self.user_model(features["query_text"])
        v = self.item_model(features["item_text"])
        return self.task(u, v)

model = RetrievalModel(user_model, item_model)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4))

# 4) Logger custom supaya output rapi (tanpa scientific notation & tanpa regularization_loss)
class CleanLogger(tf.keras.callbacks.Callback):
    def __init__(self, show_lr=True, every=1, fmt="{:.4f}", fmt_lr="{:.6f}"):
        super().__init__()
        self.show_lr = show_lr
        self.every   = every
        self.fmt     = fmt
        self.fmt_lr  = fmt_lr

    def _get_lr(self):
        try:
            # Keras 3: learning_rate bisa berupa Variable / Schedule
            lr = self.model.optimizer.learning_rate
            if hasattr(lr, "numpy"):
                return float(lr.numpy())
            return float(tf.keras.backend.get_value(lr))
        except Exception:
            return None

    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % self.every != 0:
            return
        logs = logs or {}
        msg = f"Epoch {epoch+1}/{self.params.get('epochs','?')}"
        if "loss" in logs:
            msg += f" | loss: {self.fmt.format(logs['loss'])}"
        if "val_loss" in logs:
            msg += f" | val_loss: {self.fmt.format(logs['val_loss'])}"
        if self.show_lr:
            lr = self._get_lr()
            if lr is not None:
                msg += f" | lr: {self.fmt_lr.format(lr)}"
        print(msg)

# 5) Callbacks & fit — verbose=0 supaya tidak muncul progbar default Keras
cbs = [
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss", patience=6, restore_best_weights=True, min_delta=1e-3
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss", factor=0.5, patience=3, min_lr=1e-5, verbose=0
    ),
    CleanLogger(show_lr=True, every=1, fmt="{:.4f}", fmt_lr="{:.6f}"),
    # RecallLogger(user_model, item_model, pair_val, items["gabungan"].astype(str).tolist(), ks=(5,10), every=5),
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=40,
    callbacks=cbs,
    verbose=0,   # <= penting: sembunyikan progbar bawaan
)

"""## **[6] Visualisasi kurva training**"""

plt.figure(figsize=(6,4))
plt.plot(history.history["loss"], label="train")
plt.plot(history.history["val_loss"], label="val")
plt.title("Training Curve — Retrieval (in-batch negatives)")
plt.xlabel("epoch"); plt.ylabel("loss")
plt.grid(True); plt.legend(); plt.show()

"""## **[7] Evaluasi Offline: recall@K & MRR**"""

def _embed_items(model, texts, batch=512):
    V=[]
    for i in range(0, len(texts), batch):
        V.append(model(tf.constant(texts[i:i+batch])).numpy())
    return np.vstack(V).astype("float32")

def evaluate_recall_mrr(user_model, item_model, val_df_join, items_text, ks=(1,5,10,20), batch=256):
    V = _embed_items(item_model, items_text)
    text2idx = {t:i for i,t in enumerate(items_text)}
    recalls = {k:0 for k in ks}; mrr_sum=0; n=0

    for i in range(0, len(val_df_join), batch):
        q  = val_df_join["query_text"].astype(str).values[i:i+batch]
        gt = val_df_join["item_text"].astype(str).values[i:i+batch]
        U  = user_model(tf.constant(q)).numpy()
        sims = U @ V.T
        order = np.argsort(-sims, axis=1)
        for r, t in enumerate(gt):
            gi = text2idx.get(t, -1)
            if gi < 0: continue
            ranks = order[r]
            pos   = np.where(ranks == gi)[0]
            if pos.size:
                rank = int(pos[0]) + 1
                mrr_sum += 1.0 / rank
                for k in ks:
                    if rank <= k: recalls[k] += 1
            n += 1

    return {f"recall@{k}": recalls[k]/max(1,n) for k in ks} | {"MRR": mrr_sum/max(1,n)}

items_text = items["gabungan"].astype(str).tolist()
val_join   = pair_val  # alias
metrics = evaluate_recall_mrr(user_model, item_model, val_join, items_text, ks=(1,5,10,20))
print(metrics)

"""## **[8] Export Artefak**"""

# 8.1 mapping row_idx <-> item_id
item_id_map = items[["item_id","place_name","category","city"]].copy()
item_id_map.insert(0, "row_idx", np.arange(len(items)))
item_id_map.to_csv(ART_DIR / "item_id_map.csv", index=False)
print("Saved:", (ART_DIR / "item_id_map.csv").as_posix())

# 8.2 simpan encoder
user_encoder_path = (ART_DIR / "user_encoder.keras").as_posix()
item_encoder_path = (ART_DIR / "item_encoder.keras").as_posix()
tf.keras.models.save_model(user_model, user_encoder_path, include_optimizer=False)
tf.keras.models.save_model(item_model, item_encoder_path, include_optimizer=False)
print("Saved:", user_encoder_path)
print("Saved:", item_encoder_path)

# 8.3 precompute item embeddings (N, D)
def compute_item_matrix(model, texts, batch=512):
    out = []
    for i in range(0, len(texts), batch):
        out.append(model(tf.constant(texts[i:i+batch])).numpy())
    return np.vstack(out).astype("float32")

ITEM_EMBS = compute_item_matrix(item_model, items_text, batch=512)
np.save(ART_DIR / "item_embeddings.npy", ITEM_EMBS)
print("Saved:", (ART_DIR / "item_embeddings.npy").as_posix(), "| shape:", ITEM_EMBS.shape)

# 8.4 sanity check retrieval
def retrieve_local(query: str, k=10):
    u = tf.keras.models.load_model(user_encoder_path, compile=False)  # no safe_mode arg needed
    ue = u(tf.constant([query])).numpy().astype("float32")            # (1, D)
    scores = (ue @ ITEM_EMBS.T).ravel()                               # (N,)
    topk = np.argsort(-scores)[:k]
    return [(items.loc[i, "item_id"], float(scores[i])) for i in topk]

print("Sample:", retrieve_local("pantai aceh snorkeling", k=5))

"""# **[9] Inference Sanity Check**

"""

# Paths
user_encoder_path = (ART_DIR / "user_encoder.keras")
item_encoder_path = (ART_DIR / "item_encoder.keras")
embs_path         = (ART_DIR / "item_embeddings.npy")
idmap_path        = (ART_DIR / "item_id_map.csv")
items_csv_path    = (ART_DIR / "items.csv")

# Load items & ensure item_id exists
items = pd.read_csv(items_csv_path)
if "item_id" not in items.columns:
    items["item_id"] = items.index.astype(str)
items["item_id"] = items["item_id"].astype(str)
items_text = items["gabungan"].astype(str).tolist()

# Load user encoder
user_encoder = tf.keras.models.load_model(user_encoder_path.as_posix(), compile=False)

# Load/precompute item embeddings
if embs_path.exists():
    ITEM_EMBS = np.load(embs_path).astype("float32")
else:
    # Fallback: hitung embedding item on the fly
    item_encoder = tf.keras.models.load_model(item_encoder_path.as_posix(), compile=False)
    chunks, vecs = 512, []
    for i in range(0, len(items_text), chunks):
        vecs.append(item_encoder(tf.constant(items_text[i:i+chunks])).numpy())
    ITEM_EMBS = np.vstack(vecs).astype("float32")

# Safety check dim
assert ITEM_EMBS.shape[0] == len(items), \
    f"Item embeddings N={ITEM_EMBS.shape[0]} tidak sama dengan items N={len(items)}"

# RowIdx <-> item_id map
if idmap_path.exists():
    idmap = pd.read_csv(idmap_path)
    row2id = dict(zip(idmap["row_idx"].astype(int), idmap["item_id"].astype(str)))
else:
    row2id = dict(zip(range(len(items)), items["item_id"].astype(str)))

def retrieve_local(query: str, k: int = 10):
    """Return list of (item_id, score, place_name)."""
    ue = user_encoder(tf.constant([query])).numpy().astype("float32")   # (1, D)
    scores = (ue @ ITEM_EMBS.T).ravel()                                 # (N,)
    top_idx = np.argsort(-scores)[:k]
    out = []
    for i in top_idx:
        iid  = row2id.get(int(i), items.loc[i, "item_id"])
        name = items.loc[i, "place_name"] if "place_name" in items.columns else "-"
        out.append((iid, float(scores[i]), name))
    return out

# Demo
for q in ["pantai aceh snorkeling", "gunung camping bandung"]:
    print(f"\nQ: {q}")
    for iid, sc, name in retrieve_local(q, k=5):
        print(f"  {iid:>4} | {sc:.4f} | {name}")